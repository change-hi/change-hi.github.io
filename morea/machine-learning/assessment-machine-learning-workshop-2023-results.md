---
title: "Machine Learning Workshop 2023 Assessment Results"
published: true
morea_id: machine-learning-workshop-2023-assessment-results
morea_type: reading
morea_summary: "Participant comments, instructor comments, and discussion"
morea_sort_order: 0
morea_labels:
morea_enable_toc: false
---

# Machine Learning Workshop 2023 Assessment Results

## Participant Assessment Comments

*Why select a Radial Basis Function to model CO2 concentrations at Mauna Loa? If you don't recall, just write "I don't recall".*

* I don't recall (5x)
* Data is sinusoidal
* RBF can capture complex, nonlinear relationships between the input variables (e.g., time, temperature, etc.) and the target variable (CO2 concentration).
* It would be fast and plotted smoother.

*Why might you need to use the "forward fill" capability in Pandas? If you don't recall, just write "I don't recall".* 

* I don't recall (3x).
* To fill NaN values
* To impute missing values
* To make sure that the missing data uses the correct intervals when filling it in.
* Fill gaps in dataset
* We use forward fill to fill missing values with the most recent known value, rather than imputing the missing values in some other way.
* Data interpolation
* Data is missing; its likely that data in a close time period had similar values
* The data is consecutive data (time-series).

*Has this workshop material caused you to consider making changes to the way you use machine learning in your own research and/or practice? If so, please briefly describe what changes you might make. If not, just write "No changes at the moment."*

* No changes at the moment. (4x)
* Yes, this workshop helped illuminate some of the intricacies of utilizing machine learning to analyze time series data in efficient and appropriate ways.
* Yes, exploring different workflows and adjustments and hyperparameters to models. I'm also curious to explore language processing more.
* Yes, I have a lot of timeseries data that I have not been doing much with and I am excited to start looking at predictions including using other variables. Also, it did make me more interested in NLP.
* Yes, in my process in doing machine learning research or practices, I learned that sometimes we need to think out of the box with what we're predicting rather than just making a stronger model (e.g. predicting the change rather than future values).
* I am going to try to learn how to implement pytorch instead of leaning on sklearn and keras all the time.
* More frequent use of Colab
* I will be working with collaborators on a project utilizing neural networks and this workshop was a very nice introduction
* Yes. I would like to use LSTM for practice.
* I learned appropriate way to forecast the results with time-series data.

*In general, did you find the CONTENT of this workshop to be useful?  If so, please briefly summarize up to three skills you acquired (or started to acquire) as a result of this workshop.  If not, please write "Content not useful."*

* Yes, I think Machine Learning is a very hot topic. I took a Machine Learning course in my Master's class so this workshop was not too useful in terms of learning new skills.
* Yes, I learned how to use Google collab and learned it is easier to run some codes here since the are many preinstalled libraries.
* The content was very useful and served as a good broad exposure to machine/deep learning while seeing specific use cases.
* Yes, I appreciate having a functional script with a working model i can go back and reference / test with different settings. I learned about language processing.
* Yes, especialy that we were all going along and running the code.
* Yes, modeling different parts of data, using pytorch, minmax scaling.
* The content was useful as it provided a good entry level description of how to use pytorch and sklearn.
* Visualizing complex datasets; reasoning about ML models; discussion of overtraining/overfitting pitfalls.
* Yes, it was a good refresher for the basics steps to using ML to solve time series problems. How to clean data, handle missing values, scale numbers.
* Yes. Skills: Preparing data, training the neural network model, evaluating the model through visual inspection.
* yes- learned the tradeoffs in chosing classifiers
* Useful. But I got stuck at the beginning as using collab.
* Use of pytorch, large language models, decision trees.
* It touches on useful codes and topics in machine learning.

*In general, did you find the PEDAGOGY (i.e. method of teaching) of this workshop to be appropriate?  If so, please briefly summarize one or two teaching methods you found to be useful. If not, please write "Pedagogy was not appropriate."*

* Pedagogy was not appropriate. I think the data we were working with was too complex to learn Machine Learning and Climate Data at the same time. I think starting off with a more simpler dataset and moving to complex datasets would have been a better plan.
* Yes, but fast for someone without background knowledge to follow.
* I liked that the entirety of the workshop was very hands on but not so intensive that the flow of the workshop was impacted.
* yes, i enjoyed in 'live' testing and questions to the audience. it made me reflect and think about the types of models, how to chose and how to test them.
* I believe it was appropreate and matched the ML talks I have for courses.
* Yes, sharing notebook on screen, encouraging questions.
* The method of having notebooks for us to use and play with is good because it allows us to tweak things and see how it affects the models.
* Experiential learning/learn by example; Live debugging of issues rather than perfect presentation (makes it more approachable for beginners)
* Yes, it was a well planned 2 hours.
* I thought following along with the coding live was very engaging
* Good approach, could have been explained why interpolation was used instead of dropna
* Real data implemented. The radiation section shows how to train the data is vital.
* Concrete examples showing how to apply concepts and/or ideas.
* I think sharing the codes and giving fellows time to follow up were very effective in fully understanding the instructors' contents.

*No workshop is perfect. What are one or two things you would suggest we change to improve this workshop in future?  If you can't think of anything, then write "You're wrong. This workshop was perfect."*

* You're wrong. This workshop was perfect. (2x)
* If the student speaker could speak up, that would have been great
* It would be nice to see some other ML models alongside decision trees.
* I had a hard time finding the website and datasets. i would be really helpful if there was a place it was listed as we jumped between websites - even just typed out on the big screen (sorry if there was and i missed it) also had a hard time hearing the first speaker.
* With materials provide links to the morea sites used.
* Ensure that the code posted on the website is up to date, and verify that the model(s) will perform in a way that can be explained.
* Maybe more comments throughout the code
* There were just some bugs in the code that could have been avoided beforehand if tested.
* I'm very familiar with Jupyter notebooks so I didn't have any issues, but emphasizing that folks should be ready with their on notebook or a colab notebook would be helpful.
* This is probably another workshop all together but explaining the preprocessing of the raw climate data.
* It would be better if the data would be two-dimensional.
* The "sharing of the files" before the workshop could be better by providing the link to the workshop module on Change-HI instead of being a zip file of one of the web pages that had been saved. As at least one participant was confused, it would be more helpful to state that if the code is run in a local Jupyter notebook there are some prerequisite Python packages that must be installed. This would help participants be ready to go before the workshop if they are using their local Jupyter Notebook or Lab set-up.


*Did this workshop result in any new potential collaborations (i.e. people who you might contact in future to discuss research and/or professional issues)?*

* 6 responses: No the workshop did not result in any new potential collaborations
* 7 responses: I am not sure if the workshop resulted in any new potential collaborations
* 2 responses: Yes, the workshop resulted in new collaborations.

*If the workshops resulted in any new potential collaborations, are any of them with people outside your "home" discipline (i.e. your department)?*

* 7 responses: No.
* 5 responses: I don't know
* 3 responses: Yes.

*Do you have any ideas for new workshops that would help you with your research and professional development? If so, please briefly describe. If not, please write ("No ideas at the moment.")*

* A workshop working with spatial data / rasters / images / would be super cool!
* A more in-depth workshop on tweaking neural net models
* Other workshops that explore different machine learning approaches would be great. Workshops on ChatGPT usage would also be very interesting.
* Preprocessing the climate data, or something more general; preprocessing different types of raw data including sensor data etc.

*Is there anything else you would like us to know about this workshop?*

* Thanks for the workshop
* The workshop was excellent! Thank you for taking the time to put this together.
* Good job!!
* I found it very informative. Thanks!
* I would like to know how serial correlation in time series data may critically affect on machine learning, and any appropriate solutions to deal with the issues. Thank you very much!

## Instructor Assessment Comments

*For your workshop, how would you rate the overall "success" of the workshop (i.e. your subjective sense of things like student engagement, questions, discussion, completion of activities, etc.)?*

* 2 responses: About as successful as expected.

*What aspects of your workshop did not work as well as possible, and what are your thoughts on how to improve them in future?*

* It would be great to have more audience participation. Things that could be improved: (1) spending more time generating interactive content; (2) bigger, simpler space so more people could join in person.
* Our workshops wasn't as thoroughly prepared as it could've been (especially my part). Not being able to update the github in the last 24hrs contributed. I know we had fair warning, so that was my own fault! But it would be good if more people had the ability to approve PRs. We reached out to a few people but no one had access.
* I expected to just need to port the previous year's content into Morea, but Peter wanted us to make new content instead. I think that was a good experience, but I didn't find out about that early enough. I could've avoided that situation by meeting with Peter to plan the workshop earlier.
* I couldn't get the local Morea site running on my PC laptop, although with some tinkering of the gemfile it worked on my PC desktop. I never got it working on laptop due to dependency errors.
* Live reloading never seemed to do anything for me. I still had to kill and relaunch the site every time I made a change. That was a bit tedious.
* Since our topic involved so much code, it felt like the ability to make it interactive was a bit limited. Including line by line of code that users need to copy into a notebook felt more tedious than educational. But it seems like including a pre-solved jupyter notebook instead would've removed any need for user participation. I'm not sure what the answer is, maybe we could've done it more similar to the data wrangling workshop.

*What aspects of your workshop do you feel worked well (so that we know to encourage their use in future workshops when appropriate)?*

* Audience seemed interested in the topic.
* Morea is easy to work with once you get your environment running.
* Michelle taking care of zoom, camera, mics etc is nice, so we can just focus on the content.

*Is there anything else you would like us to know about your workshop? Are there ways we can better support your instruction in future?*

* The morea interface is nice and lends itself pretty well to workshops. I especially liked the ability to link to  previous modules and create call outs that gave more information on the topic.  However, it adds a lot of overhead to building workshops compared to doing everything in jupyter notebooks + github. Getting ruby configured on my machine was a hassle (even though I already use it for my jekyll github page).
* When Peter and I were thinking about the topics we wanted to cover, his main questions for me were about who the audience would be and what they will be interested in. I didn't have much of an answer. Most workshops have small attendance, mostly just the fellows, and it feels like the fellows don't benefit much from intro-level coding tutorials. On the other hand, we get some external attendees, and sometimes a lot of them (like this time). Those people are a bit of a mystery. With them it's probably best to give a very intro-level workshop? I don't know.

*Do you have ideas for new workshops (don't worry, we'll find someone else to lead them!) If so, please jot down your idea here. If not, write "Nothing at the moment".*

* There is only one explicitly machine learning workshop this year, but there are a lot of different specialized topics that people might be interested. e.g. ML object detection in satellite images. Speeding up human data annotation. Making use of LLMs. Topic modeling. Hyperparameter optimization. Distributed neural network training. Model compression. Model interpretability tools. Model de-biasing tools.
* Maybe something about measuring and communicating the significance of results. Very quick "stats refresher for researchers" and with some advice about how to avoid cherrypicking etc.
* There are also tooling workshops that could be done. Running VisualStudio on the cluster. Using copilot.


## Discussion and recommendations

The initial questions assess a couple of the learning outcomes.  The responses provide partial evidence that the learning outcomes assessed were achieved. It may be that the first question (in particular) was not appropriate for assessing learning outcomes. 

Participant feedback seems to indicate that the pacing of this workshop was reasonable. 

Machine Learning is a very interesting topic and there is a lot of interest in expanding the treatment of this topic in some manner in future years.

It seems that the workshop participants ranged greatly in background: from some who have taken ML courses in the past to those with no prior experience. Satisfying both audiences is challenging, but you seem to have done a pretty good job. 

Jekyll is notorious for being difficult to set up on Windows. Since Morea depends upon Jekyll, this can be a problem. Fortunately, I will soon be documenting a cloud-based approach based upon GitPod.io which will significantly simplify Morea use for Windows (and even Unix) users. 

Recommendations for next time:

1. There was interesting constructive feedback on potential improvements; these can be considered for the next time this workshop is run.
2. Think about ways to change the in-class interaction so that more collaborative connections between folks in different disciplines occurs.
3. Find better questions for assessing learning outcomes. 
