---
title: "FAIR Workshop 2022 Assessment Results"
published: true
morea_id: fair-workshop-2022-assessment-results
morea_type: reading
morea_summary: "Participant comments, instructor comments, and discussion"
morea_sort_order: 0
morea_labels:
morea_enable_toc: false
---

# FAIR Workshop 2022 Assessment Results

## Participant Assessment Comments

*Do you think FAIR (Findable, Accessible, Interoperable, Reusable) concepts are important for data management? If so, why? If not, why not? If you don't recall what FAIR concepts are, just write "I don't recall".*

* Yes absolutely important for people anticipating to publish data.
* Absolutely and I think that RJ (presenter) said it best, there needs to be a standardized means of data management. Whether this exact system is correct or not, the consistency in the system enables us to better organize and manage data.
* Yes, having a quality control that allows for accurate tracing and replicability is important in scaling up any research and is important to the scientific process
* Yes; we need to make sure that data we generate is able to be found and utilized in the future by other researchers, and also put in places that it will not get lost as time goes on.
* Yes, I do. I think carefully considering and appropriately implementing FAIR principles will improve the outcomes of scientific research across disciplines.
* FAIR concepts are important for data management. These concepts allow for replicability and transparency of research which helps push the field forward as a whole.

*Can you recall one barrier or risk associated with the adoption of an open source project? If so, what is it? If not, write "I don't recall".* 

* A risk is that the software written can be open sourced and can expose the limits of the code to the public. Also public data can be harmful to native people or people who want their information to be private.
* One risk mentioned is security.
* not all open source solutions are necessarily the best solution; for example, if someone uses Jupyter notebook that someone else created, there could be sections of the notebook that contain dead code or are not applicable to the current releases of the software used
* I don't recall.
* There is a risk of other researchers not involved with the project interpreting and using results without the full context that a contributing researcher might have.
* A risk associated with the adoption of an open source project is possible intellectual property theft.

*In your own words, please summarize the general process of identifying a FAIR dataset. If you don't recall, just write "I don't recall".* 

* You assess the 4 components that make a dataset FAIR.
* There is a tool we used in the workshop that guides one through the process of identifying FAIR, but the general idea is to adhere to the FAIR acronym.
* Findable, Accessible, Interoperable, Reusable
* Is it organized, is it labeled using best practices?
* Finding everything needed to replicate the project in one place; managing the accessibility of the data; collecting the data in a way that is duplicatable and shareable; usage licenses
* Look to make sure it is Findable, Accessible, Interoperable, and Reusable.
* We want to evaluate the project on its findability, accessibility, interoperability, and reusability.
* The general process is largely related to the metadata of a data set. Essentially, we want to establish each of the FAIR principles by evaluating documentation and transparency of the dataset.

*Has this workshop material caused you to consider making changes to your own data management habits? If so, please briefly describe what changes you might make. If not, just write "No changes at the moment."*

* No changes at the moment.
* Yes, it makes me value metadata and its organization more.
* yes - paying attention to names and making sure that the data is understandable at all stages so that I can more easily collaborate with others
* Yes; I will likely be spending more up front time in future projects that work with data making sure that we prepare our data to be FAIR.
* Yes it has. It made me consider certain data formats, appropriate identifying features, metadata, and data security.
* This workshop has compelled me to consider FAIR data management concepts to improve replicability of my work.

*In general, did you find the CONTENT of this workshop to be useful?  If so, please briefly summarize up to three skills you acquired (or started to acquire) as a result of this workshop.  If not, please write "Content not useful."*

* Very much so. I can apply the concepts of how to make my data FAIR when I publish.
* Yes, the main thing is that it helped me better understand metadata and best practices to make my own work FAIR
* yes - 1) introduced to FAIR principles, 2) practiced applying FAIR principles, 3) introduced to Hydroshare
* Yes, I started to acquire the ability to recognize and categorize information and metadata, recognize threats from a security perspective, and work on different methods for formatting data to be machine and human readable.
* Yes, I did. I acquired a more broad mindset regarding the "fairness" of conducting research and managing data. I further considered the accessibility of my work given the nature of the inputs to my research. I further considered security of my data.
* The content of the workshop was useful. Three skills that I acquired are: understanding of FAIR principles, ability to evaluate whether a dataset is FAIR, and understanding the necessary formats to make my dataset FAIR.

*In general, did you find the PEDAGOGY (i.e. method of teaching) of this workshop to be appropriate?  If so, please briefly summarize one or two teaching methods you found to be useful. If not, please write "Pedagogy was not appropriate."*

* Yes very appropriate.
* Yes, but it felt like too much content for the 2 hour session. Lots was skipped or glossed over.
* the written out definitions and exercises were useful in following along with the material
* Yes; having both the information available in a video and online format made it easy to follow along, and having the split out short exercises worked well for experiential learning.
* I believed the pedagogy was appropriate. I liked the integrated exercises along with the presentation material.
* The pedagogy was appropriate. The examples with evaluations of datasets helped drive my understanding of the material.

*No workshop is perfect. What are one or two things you would suggest we change to improve this workshop in future?  If you can't think of anything, then write "You're wrong. This workshop was perfect."*

* You're wrong. Workshop is perfect.
* Perhaps cut down the content and exercises. It is an important topic with complex ideas to learn so it takes a while to teach and absorb.
* maybe some illustrations to add to the descriptions; but not necessary - would just be extra
* I think that it would be useful to front load the information ahead of the workshop a bit more, as in say "You may wish to read this ahead of time".
* I suggest having less dense text on the screen when presenting. It is useful for a self-guided workshop, but when there are presenters, I prefer more visual aids and graphics with bullet points to large blocks of text.
* I would have liked to look at some of the examples; however, we were rushed for time.

*Did this workshop result in any new potential collaborations (i.e. people who you might contact in future to discuss research and/or professional issues)?*

* 5 responses: I am not sure if the workshop resulted in
any new potential collaborations
* 1 response: Yes, the workshop resulted in a handful
(less than 5) potential collaborations

*If the workshops resulted in any new potential collaborations, are any of them with people outside your "home" discipline (i.e. your department)?*

* 1 response: No.
* 2 responses: Yes, at least one is outside
* 3 responses: I don't know

## Instructor Assessment Comments

*For your workshop, how would you rate the overall "success" of the workshop (i.e. your subjective sense of things like student engagement, questions, discussion, completion of activities, etc.)?*

* 2 responses: About as successful as expected.

*What aspects of your workshop did not work as well as possible, and what are your thoughts on how to improve them in future?*

* I think we should cull a bit more content up front and perhaps divide the FAIR assesment exercise for the first data set across the 4 FAIR sections to add a more related activity/exercise in there and some more interaction in the first half of the workshop.

* Not enough time to explain all the concepts and content. Need more time or reduce the content available. Maybe label more things as 'optional' so we know which ones are really important to go over. It might be nice to have a checklist for each section se we know which ones we already went over? But this would require accounts for each student/instructor so maybe not.

*What aspects of your workshop do you feel worked well (so that we know to encourage their use in future workshops when appropriate)?*

* The interactive discussions and exercises seem to go well.
* Having the schedule up was really good to keep us on track.

*Is there anything else you would like us to know about your workshop? Are there ways we can better support your instruction in future?*

* If we do hybrid we should better look at breakouts in zoom maybe.
* But it would have been nice to maybe have a timer? Like maybe a navigation menu on the left side of the screen where each section is labeled by the time we are supposed to switch to the next section? Or maybe put it on the top.

## Discussion and recommendations

The initial questions assess several of the learning outcomes.  The responses provide evidence that the learning outcomes assessed were, in general, achieved. The workshop appeared to be rushed, and there was little evidence that participants made connections across disciplines. 

Recommendations for next time:

1. Review Fall 2022 screencast to determine which sections were "rushed", and reduce content in those sections.
2. Increase font size of screen during workshop so that text is less dense. Consider more bullet point style when appropriate.
3. Implement ways to increase ability of participants to learn about each other's disciplines and form interdisciplinary connections.  Could be through an initial ice-breaker exercise or integrated into small group exercises.
4. Consider Zoom breakout rooms or other ways to integrate online participants.
5. Create a final 5 minute "What's next?" section just prior to the assessment. This section provides links to more advanced material and/or material removed from the "rushed" sections. This is intended for independent "post-workshop" work by participants, not for coverage during the workshop.  
